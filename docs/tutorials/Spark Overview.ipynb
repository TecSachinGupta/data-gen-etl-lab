{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e29455-a93d-4818-a711-4dd35235e9c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "\n",
    "1. Unified analytics engine for large-scale data processing\n",
    "2. Optimized engine that supports general execution graphs\n",
    "3. Provides high-level APIs for Java, Scala, Python, and R\n",
    "4. Rich set of higher-level tool i.e.\n",
    "   1. **Spark SQL** for SQL and structured data processing\n",
    "   2. **Pandas API on Spark** for pandas workloads\n",
    "   3. **MLlib** for machine learning\n",
    "   4. **GraphX** for graph processing \n",
    "   5. **Structured Streaming** for incremental computation and stream processing\n",
    "\n",
    "## Features\n",
    "![Features of Apache Spark](https://miro.medium.com/max/4096/1*6e2oYaa6rtNNRHbmbenETQ.png \"Features of Apache Spark\")\n",
    "1. **Speed:** Spark performs up to 100 times faster than MapReduce for processing large amounts of data. It is also able to divide the data into chunks in a controlled way.\n",
    "2. **Powerful Caching:** Powerful caching and disk persistence capabilities are offered by a simple programming layer.\n",
    "3. **Deployment:** __Mesos__, __Hadoop via YARN__, or __Sparkâ€™s own cluster manager__ can all be used to deploy it.\n",
    "4. **Real-Time:** Because of its in-memory processing, it offers real-time computation and low latency.\n",
    "5. **Polyglot:** Spark provides high-level APIs in Java, Scala, Python, and R. Spark also provides a command-line interface for Scala and Python.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The Apache Spark architecture consists of two main abstraction layers:\n",
    "1. **Resilient Distributed Datasets (RDD):**\n",
    "   - Building block of Spark application\n",
    "   - Stands for\n",
    "      - **Resilient:** Fault tolerant and is capable of rebuilding data on failure\n",
    "      - **Distributed:** Distributed data among the multiple nodes in a cluster\n",
    "      - **Dataset:** Collection of partitioned data with values\n",
    "   - A layer of abstracted data over the distributed collection\n",
    "   - Immutable in nature and follows lazy transformations.\n",
    "   - Enables to recheck data in the event of a failure. \n",
    "   - two types of operation can performed, they are\n",
    "      - **Transformations:** They are the operations that are applied to create a new RDD.\n",
    "      - **Actions:** They are applied on an RDD to instruct Apache Spark to apply computation and pass the result back to the driver.\n",
    "2. **Directed Acyclic Graph (DAG):**\n",
    "   - **Directed:** the operations are executed in a specific order\n",
    "   - **Acyclic:** no loops or cycles in the execution plan\n",
    "   - Represents the logical execution plan\n",
    "   - Allows Spark to break down a large-scale data processing job into smaller, independent tasks that can be executed in parallel\n",
    "   - Helps in identifying any potential bottlenecks or performance issues\n",
    "\n",
    "### Working Architecture\n",
    "![](https://www.simplilearn.com/ice9/free_resources_article_thumb/spark-execution-architecture-process.JPG )\n",
    "\n",
    "1. **Spark Driver or Driver Program:**\n",
    "   - A\n",
    "2. **Cluster Manager:**\n",
    "   - A\n",
    "3. **Worker Node or Executors:**\n",
    "   - A\n",
    "\n",
    "## Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b362364-18f5-49b6-a271-6d725bf84de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Terminologies\n",
    "1. Lineage\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5deea814-7cd6-4ad9-a22f-05bfef0f9598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Spark Connect\n",
    "https://spark.apache.org/docs/latest/spark-connect-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0c54033-36d9-4c6c-9543-01da7356d18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#References\n",
    "1. https://spark.apache.org/docs/latest/\n",
    "2. https://www.interviewbit.com/blog/apache-spark-architecture/\n",
    "3. https://www.edureka.co/blog/spark-architecture/\n",
    "4. https://sparkbyexamples.com/spark/what-is-dag-in-spark/\n",
    "5. https://medium.com/@tomhcorbin/understanding-apache-spark-part-1-spark-architecture-21c347bf622b\n",
    "6. https://towardsdatascience.com/apache-spark-multi-part-series-spark-architecture-461d81e24010\n",
    "7. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Overview",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}