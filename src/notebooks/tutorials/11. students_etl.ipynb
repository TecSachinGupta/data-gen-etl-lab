{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Version 1"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Version 1",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "from delta import *"
   ],
   "execution_count":1,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"gVo10bB0H14sa7zbwfnEpc",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "SPARK_CONFIGS = {\n",
    "    \"spark.sql.sources.partitionOverwriteMode\": \"dynamic\",\n",
    "}"
   ],
   "execution_count":2,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"T2L7AtFBa3i0OCLxRxTs5V",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Global variables or constants\n",
    "INPUT_PATH = \"\/data\/workspace_files\/input\/\"\n",
    "OUTPUT_PATH = \"\/data\/workspace_files\/output\/\""
   ],
   "execution_count":3,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"OpSRuGHeatpbVKLRmj7EXh",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# configuring the logger to print logs\n",
    "log_format1 = '%(asctime)s [%(levelname)-8s] <PID %(process)d:%(processName)s> %(name)s.%(funcName)s: %(message)s'\n",
    "log_format2 = '%(asctime)s [%(levelname)-8s] %(name)s.%(funcName)s: %(message)s'\n",
    "log_format3 = '%(asctime)s [%(levelname)-8s] [%(processName)s] %(name)s: %(message)s'\n",
    "\n",
    "formatter = logging.Formatter(log_format3, datefmt='%d-%b-%Y %H:%M:%S')\n",
    "\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler(OUTPUT_PATH + '\/logs\/student_etl_pyspark_stdout.log', \"a\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger(\"Students Data ETL\")\n",
    "\n",
    "logger.handlers.clear()\n",
    "\n",
    "logger.addHandler(console)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ],
   "execution_count":4,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"9VPflyOJK7ioAC4qaqqYEO",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"PySpark Excercises\") \\\n",
    "                    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,io.delta:delta-spark_2.12:3.1.0\") \\\n",
    "                    .config(\"spark.sql.warehouse.dir\", \"\/data\/workspace_files\/warehouse\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ],
   "execution_count":5,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"Qy5af2r5Xc81DQDXfxSmkb",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "for key in SPARK_CONFIGS:\n",
    "    spark.conf.set(key, SPARK_CONFIGS[key])"
   ],
   "execution_count":6,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"0i4vTv0snQmgoZ9f7yMlgo",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## 01. Read Data from Files"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"htKm6FZwX6KeBrmDoyv37E",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "classes_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/classes.json\")\n",
    "students_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/students_data.json\")\n",
    "address_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/student_address*.json\")\n",
    "phone_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/student_phone.json\")\n",
    "marks_df = spark.read.csv(INPUT_PATH + \"students\/student_marks.csv\", header=True, schema=\"student_id long, class_id long, subject string, marks_obtained integer\")"
   ],
   "execution_count":7,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"KacCx984mhTCrHIwiviOOU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## 02. Transform Data"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"dqZr2BhOJarSX6VHyRMGVP",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "address_df = address_df.groupBy(address_df.student_id) \\\n",
    "                        .agg(collect_set(struct( \\\n",
    "                                                address_df.type, \\\n",
    "                                                address_df.street_address.alias(\"address_line1\"), \\\n",
    "                                                address_df.address_line_2, \\\n",
    "                                                address_df.city, \\\n",
    "                                                address_df.state, \\\n",
    "                                                address_df.country, \\\n",
    "                                                address_df.postal_code, \\\n",
    "                                                struct(address_df.latitude, address_df.longitude).alias(\"location\") \\\n",
    "                                               )).alias(\"address\"))"
   ],
   "execution_count":8,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"hwexkJCwNpamQ8UZviJ8Lo",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "phone_df = phone_df.groupBy(phone_df.student_id) \\\n",
    "                    .agg(collect_set(struct( \\\n",
    "                                            phone_df.type, \\\n",
    "                                            phone_df.phone_number, \\\n",
    "                                            phone_df.email \\\n",
    "                                           )).alias(\"contact_detail\"))"
   ],
   "execution_count":8,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"MkWfaNkezSHY37FUeKxYvY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "marks_df = marks_df.withColumn(\"total_marks_obtained\", sum(marks_df.marks_obtained).over(Window.partitionBy(\"student_id\", \"class_id\"))) \\\n",
    "                    .withColumn(\"total_marks\", sum(lit(100)).over(Window.partitionBy(\"student_id\", \"class_id\")))\n",
    "marks_df = marks_df.groupBy(marks_df.student_id, marks_df.class_id) \\\n",
    "                    .agg( \\\n",
    "                         first(marks_df.total_marks_obtained, ignorenulls=True).alias(\"total_marks_obtained\"), \\\n",
    "                         first(marks_df.total_marks, ignorenulls=True).alias(\"total_marks\"), \\\n",
    "                         collect_set(struct( \\\n",
    "                                            marks_df.subject, \\\n",
    "                                            marks_df.marks_obtained \\\n",
    "                                           )).alias(\"progress_report\") \\\n",
    "                        )"
   ],
   "execution_count":9,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"YhDwJDv4qUxIO15UVVT1lK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class_marks_df = marks_df.join(classes_df, [marks_df.class_id == classes_df.class_id], \"left\") \\\n",
    "                         .drop(classes_df.class_id)"
   ],
   "execution_count":10,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"pBjqZAGGK5eWzMFwFhLZGs",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class_marks_df = class_marks_df.groupBy(class_marks_df.student_id) \\\n",
    "                               .agg( \\\n",
    "                                    collect_set(struct( \\\n",
    "                                                       class_marks_df.standard, \\\n",
    "                                                       class_marks_df.section, \\\n",
    "                                                       class_marks_df.total_marks_obtained, \\\n",
    "                                                       ((class_marks_df.total_marks_obtained \/ class_marks_df.total_marks) * 100).alias(\"percent\"), \\\n",
    "                                                       class_marks_df.progress_report \\\n",
    "                                    )).alias(\"classes\") \\\n",
    "                                   )"
   ],
   "execution_count":11,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"hPtPjku5mhTJJqVd9CqepI",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df = students_df.join(phone_df, [students_df.student_id == phone_df.student_id], \"left\") \\\n",
    "                      .join(address_df, [students_df.student_id == address_df.student_id], \"left\") \\\n",
    "                      .join(class_marks_df, [students_df.student_id == class_marks_df.student_id], \"left\") \\\n",
    "                      .drop(phone_df.student_id, address_df.student_id, class_marks_df.student_id)"
   ],
   "execution_count":12,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"u8xJwwTpp5pnMb9GaPHptM",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df = final_df.select( \\\n",
    "                           final_df.student_id.alias(\"_id\"), \\\n",
    "                           struct( \\\n",
    "                                  struct(final_df.first_name, final_df.last_name).alias(\"name\"), \\\n",
    "                                  final_df.date_of_birth, \\\n",
    "                                  final_df.gender, \\\n",
    "                                  final_df.hair_color, \\\n",
    "                                  final_df.eye_color, \\\n",
    "                                  final_df.height, \\\n",
    "                                  final_df.weight, \\\n",
    "                                  final_df.hobbies, \\\n",
    "                                  final_df.library_card_number, \\\n",
    "                                  final_df.meal_plan, \\\n",
    "                                  final_df.sports_team, \\\n",
    "                                  final_df.club_membership, \\\n",
    "                                  final_df.tuition_paid, \\\n",
    "                                  final_df.financial_aid, \\\n",
    "                                  final_df.housing_status, \\\n",
    "                                  final_df.contact_detail, \\\n",
    "                                  final_df.address, \\\n",
    "                                  final_df.classes \\\n",
    "                                 ).alias(\"student\") \\\n",
    "                          )"
   ],
   "execution_count":13,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"VjBUugGrTMAnjunk0jytZG",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## 03. Load data"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"0SbflaW7vMg9i8uMioKcN0",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df.show(5, truncate=False)"
   ],
   "execution_count":15,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_id|student                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|6  |{{Karl, Mattei}, 1991-08-19, Male, brown, brown, 71.99, 166.54, cooking, 30056, Partial, Volleyball, etc., Yuan Renminbi, true, false, NULL, [{NULL, 8675 Porter Road, Apt 662, Wufeng, NULL, China, NULL, {30.199688, 110.674706}}], [{11, Arts, 495, 82.5, [{Geography, 89}, {Literature, 100}, {Political Science, 56}, {History, 98}, {Sociology, 96}, {English, 56}]}, {4, A, 420, 70.0, [{Hindi, 60}, {Social Studies, 68}, {Mathematics, 71}, {Computer, 96}, {Science, 64}, {English, 61}]}, {8, C, 480, 80.0, [{Hindi, 95}, {Science, 86}, {Social Studies, 70}, {Mathematics, 59}, {Computer, 73}, {English, 97}]}, {1, A, 402, 67.0, [{Social Studies, 58}, {Science, 89}, {English, 58}, {Hindi, 53}, {Mathematics, 74}, {Computer, 70}]}, {6, B, 443, 73.83333333333333, [{Social Studies, 72}, {Hindi, 88}, {English, 64}, {Mathematics, 85}, {Computer, 67}, {Science, 67}]}, {7, B, 476, 79.33333333333333, [{Mathematics, 89}, {Computer, 90}, {Hindi, 55}, {Social Studies, 73}, {English, 74}, {Science, 95}]}, {10, B, 485, 80.83333333333333, [{Science, 76}, {Hindi, 70}, {English, 88}, {Computer, 96}, {Mathematics, 86}, {Social Studies, 69}]}, {9, B, 517, 86.16666666666667, [{Science, 91}, {Hindi, 83}, {Computer, 87}, {English, 89}, {Mathematics, 98}, {Social Studies, 69}]}, {3, B, 483, 80.5, [{Hindi, 90}, {Science, 72}, {Computer, 80}, {Social Studies, 60}, {English, 90}, {Mathematics, 91}]}, {12, Arts, 480, 80.0, [{Literature, 86}, {Sociology, 71}, {History, 71}, {Geography, 62}, {Political Science, 100}, {English, 90}]}, {5, B, 503, 83.83333333333334, [{Social Studies, 64}, {Mathematics, 67}, {Hindi, 99}, {Science, 100}, {English, 96}, {Computer, 77}]}, {2, B, 419, 69.83333333333334, [{Social Studies, 72}, {English, 86}, {Mathematics, 52}, {Computer, 63}, {Science, 74}, {Hindi, 72}]}]}                                                                                                                                                                                                                                                       |\n",
      "|5  |{{Nealy, Briffett}, 1992-04-30, Male, brown, green, 81.63, 212.44, reading, 91560, None, Soccer, Debate Team, Zloty, true, true, NULL, [{NULL, 0 Reinke Center, Suite 74, Castres, Midi-Pyrénées, France, 81109 CEDEX, {43.7058177, 2.1365748}}, {Permanent, 544 Manufacturers Way, Apt 144, Youludu Sibage, NULL, China, NULL, {41.548437, 82.433337}}, {NULL, 7 Bobwhite Crossing, Suite 1, Dongshangguan, NULL, China, NULL, {36.740034, 103.26311}}], [{3, C, 397, 66.16666666666666, [{English, 62}, {Hindi, 72}, {Science, 57}, {Social Studies, 84}, {Computer, 58}, {Mathematics, 64}]}, {1, D, 429, 71.5, [{Hindi, 65}, {English, 73}, {Mathematics, 90}, {Computer, 53}, {Social Studies, 90}, {Science, 58}]}, {4, D, 435, 72.5, [{Computer, 90}, {Hindi, 81}, {Science, 59}, {Social Studies, 73}, {English, 50}, {Mathematics, 82}]}, {6, C, 432, 72.0, [{Science, 71}, {English, 50}, {Hindi, 78}, {Social Studies, 81}, {Mathematics, 55}, {Computer, 97}]}, {5, A, 462, 77.0, [{Social Studies, 61}, {Mathematics, 75}, {English, 79}, {Computer, 93}, {Hindi, 62}, {Science, 92}]}, {9, B, 482, 80.33333333333333, [{Science, 91}, {Mathematics, 57}, {Social Studies, 73}, {Hindi, 93}, {Computer, 73}, {English, 95}]}, {8, D, 479, 79.83333333333333, [{Computer, 92}, {Hindi, 83}, {Science, 84}, {Social Studies, 95}, {Mathematics, 64}, {English, 61}]}, {7, A, 432, 72.0, [{Science, 60}, {English, 81}, {Computer, 74}, {Social Studies, 92}, {Hindi, 53}, {Mathematics, 72}]}, {11, Arts, 516, 86.0, [{History, 85}, {Political Science, 99}, {English, 77}, {Geography, 97}, {Literature, 79}, {Sociology, 79}]}, {10, A, 440, 73.33333333333333, [{Science, 71}, {Social Studies, 64}, {Computer, 52}, {English, 94}, {Mathematics, 86}, {Hindi, 73}]}, {12, Arts, 520, 86.66666666666667, [{History, 88}, {Literature, 86}, {English, 82}, {Political Science, 96}, {Geography, 72}, {Sociology, 96}]}, {2, B, 416, 69.33333333333334, [{Hindi, 79}, {Science, 76}, {English, 77}, {Social Studies, 52}, {Mathematics, 62}, {Computer, 70}]}]}                                      |\n",
      "|1  |{{Jaclin, Cadagan}, 2002-09-24, Female, black, brown, 86.84, 206.95, hiking, 54365, None, Soccer, Photography Club, Peso, false, false, [{Primary, 339-225-4748, adenzilowpx@sitemeter.com}, {Primary, 511-993-2224, abagenal8m@wordpress.org}], [{Correspondance, 56593 Atwood Court, Room 323, Domaradz, NULL, Poland, 36-230, {49.7863419, 21.9461429}}], [{12, Commerce, 388, 77.60000000000001, [{English, 73}, {Mathematics, 79}, {Business Studies, 81}, {Accountancy, 88}, {Economics, 67}]}, {1, D, 447, 74.5, [{Social Studies, 72}, {Computer, 54}, {English, 98}, {Science, 75}, {Mathematics, 92}, {Hindi, 56}]}, {8, B, 384, 64.0, [{Computer, 90}, {Science, 70}, {Social Studies, 51}, {Mathematics, 68}, {English, 55}, {Hindi, 50}]}, {11, Commerce, 369, 73.8, [{Mathematics, 89}, {English, 81}, {Accountancy, 68}, {Economics, 74}, {Business Studies, 57}]}, {2, D, 435, 72.5, [{Computer, 92}, {Social Studies, 64}, {Hindi, 77}, {Mathematics, 60}, {English, 84}, {Science, 58}]}, {7, B, 456, 76.0, [{Computer, 66}, {Social Studies, 56}, {Mathematics, 84}, {Hindi, 94}, {Science, 95}, {English, 61}]}, {5, D, 498, 83.0, [{Hindi, 95}, {Computer, 94}, {English, 79}, {Mathematics, 59}, {Science, 95}, {Social Studies, 76}]}, {6, B, 506, 84.33333333333334, [{English, 98}, {Computer, 99}, {Mathematics, 51}, {Science, 79}, {Social Studies, 95}, {Hindi, 84}]}, {10, B, 432, 72.0, [{Social Studies, 64}, {Hindi, 70}, {English, 74}, {Mathematics, 68}, {Science, 92}, {Computer, 64}]}, {9, C, 491, 81.83333333333334, [{Science, 99}, {Social Studies, 85}, {Hindi, 99}, {English, 83}, {Computer, 57}, {Mathematics, 68}]}, {4, C, 446, 74.33333333333333, [{Science, 52}, {English, 79}, {Hindi, 61}, {Computer, 97}, {Social Studies, 93}, {Mathematics, 64}]}, {3, B, 494, 82.33333333333334, [{Computer, 92}, {Hindi, 81}, {Mathematics, 73}, {English, 93}, {Science, 62}, {Social Studies, 93}]}]}                                                                                                                                                                  |\n",
      "|3  |{{Foss, Gianninotti}, 1996-09-08, Male, brown, green, 78.26, 206.06, reading, 15881, Full, Basketball, Debate Team, Zloty, true, false, [{Secondary, 956-358-2596, rpenwrightam@tuttocitta.it}], [{Correspondance, 2320 Montana Park, PO Box 86589, Infonavit, San Luis Potosi, Mexico, 79960, {19.307033, -99.1675842}}, {Permanent, 5636 Hoepker Place, Apt 54, Heydərabad, NULL, Azerbaijan, NULL, {39.7231403, 44.8476661}}, {NULL, 81996 Northland Trail, 14th Floor, Chitungwiza, NULL, Zimbabwe, NULL, {-18.0197815, 31.067907}}], [{6, B, 415, 69.16666666666667, [{Computer, 63}, {Mathematics, 51}, {Social Studies, 68}, {Science, 100}, {English, 77}, {Hindi, 56}]}, {2, A, 508, 84.66666666666667, [{Social Studies, 99}, {Mathematics, 65}, {English, 98}, {Hindi, 59}, {Science, 98}, {Computer, 89}]}, {3, D, 453, 75.5, [{Social Studies, 66}, {Computer, 83}, {Hindi, 60}, {Science, 100}, {Mathematics, 64}, {English, 80}]}, {12, Commerce, 340, 68.0, [{Mathematics, 89}, {Economics, 68}, {Business Studies, 61}, {Accountancy, 59}, {English, 63}]}, {5, A, 462, 77.0, [{Hindi, 54}, {Mathematics, 93}, {English, 74}, {Computer, 67}, {Science, 94}, {Social Studies, 80}]}, {8, B, 447, 74.5, [{Hindi, 81}, {Science, 80}, {Mathematics, 52}, {Computer, 91}, {English, 84}, {Social Studies, 59}]}, {11, Commerce, 402, 80.4, [{Mathematics, 75}, {Business Studies, 99}, {Accountancy, 81}, {Economics, 77}, {English, 70}]}, {4, D, 466, 77.66666666666666, [{English, 99}, {Science, 91}, {Computer, 79}, {Social Studies, 52}, {Mathematics, 95}, {Hindi, 50}]}, {7, C, 411, 68.5, [{Social Studies, 64}, {Hindi, 64}, {Mathematics, 64}, {English, 71}, {Computer, 98}, {Science, 50}]}, {9, B, 534, 89.0, [{Social Studies, 98}, {Science, 74}, {Mathematics, 96}, {Hindi, 94}, {Computer, 77}, {English, 95}]}, {10, B, 434, 72.33333333333334, [{Hindi, 65}, {Computer, 66}, {Science, 71}, {English, 88}, {Social Studies, 56}, {Mathematics, 88}]}, {1, C, 429, 71.5, [{Social Studies, 89}, {Hindi, 63}, {Computer, 51}, {English, 64}, {Science, 67}, {Mathematics, 95}]}]}|\n",
      "|2  |{{Ebony, Coatsworth}, 1997-11-19, Female, brown, green, 71.98, 169.44, cooking, 54181, None, Volleyball, etc., Dollar, false, true, [{Emergency, 330-571-7344, pdeath5z@alexa.com}], [{Permanent, 653 Eliot Hill, Room 485, Irecê, NULL, Brazil, 44900-000, {-11.303555, -41.8561503}}], [{1, D, 402, 67.0, [{Science, 59}, {English, 50}, {Computer, 91}, {Mathematics, 71}, {Social Studies, 78}, {Hindi, 53}]}, {6, C, 467, 77.83333333333333, [{Social Studies, 85}, {English, 76}, {Hindi, 70}, {Mathematics, 99}, {Computer, 87}, {Science, 50}]}, {3, B, 470, 78.33333333333333, [{Mathematics, 58}, {Science, 82}, {Hindi, 97}, {Computer, 72}, {Social Studies, 71}, {English, 90}]}, {7, B, 455, 75.83333333333333, [{Science, 89}, {English, 81}, {Computer, 74}, {Social Studies, 79}, {Hindi, 76}, {Mathematics, 56}]}, {11, Arts, 424, 70.66666666666667, [{Literature, 60}, {History, 90}, {Sociology, 59}, {Geography, 53}, {Political Science, 99}, {English, 63}]}, {5, B, 425, 70.83333333333334, [{Social Studies, 65}, {Hindi, 54}, {Mathematics, 75}, {Computer, 69}, {English, 98}, {Science, 64}]}, {4, C, 531, 88.5, [{English, 98}, {Mathematics, 68}, {Computer, 93}, {Hindi, 98}, {Science, 94}, {Social Studies, 80}]}, {10, B, 427, 71.16666666666667, [{Hindi, 63}, {Science, 85}, {English, 67}, {Mathematics, 85}, {Computer, 56}, {Social Studies, 71}]}, {8, D, 370, 61.66666666666667, [{Mathematics, 83}, {English, 51}, {Hindi, 69}, {Computer, 62}, {Social Studies, 52}, {Science, 53}]}, {12, Arts, 453, 75.5, [{Literature, 60}, {History, 73}, {Political Science, 81}, {Sociology, 55}, {Geography, 87}, {English, 97}]}, {2, A, 414, 69.0, [{Social Studies, 73}, {English, 68}, {Hindi, 100}, {Science, 51}, {Computer, 58}, {Mathematics, 64}]}, {9, B, 358, 59.66666666666667, [{Computer, 54}, {Science, 59}, {Social Studies, 56}, {Hindi, 62}, {English, 71}, {Mathematics, 56}]}]}                                                                                                                                                                            |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"CnMpVR4pnND0Ams4TUf5yA",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "url = \"mongodb+srv:\/\/admin:Admin1234567890@datacluster.v8sn2hp.mongodb.net\/?retryWrites=true&w=majority&appName=DataCluster\"\n",
    "database = \"dummy\"\n",
    "collection = \"students\""
   ],
   "execution_count":14,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"4IMNGk3hMxg44E3h8PiGge",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df.write.format(\"mongodb\") \\\n",
    "              .mode(\"append\") \\\n",
    "              .option(\"mode\", \"dropMalformed\") \\\n",
    "              .option(\"connection.uri\", url) \\\n",
    "              .option(\"database\", database) \\\n",
    "              .option(\"collection\", collection) \\\n",
    "              .option(\"maxBatchSize\", 10000) \\\n",
    "              .save()"
   ],
   "execution_count":15,
   "outputs":[
    {
     "ename":"Py4JJavaError",
     "evalue":"Py4JJavaError: An error occurred while calling o350.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https:\/\/spark.apache.org\/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base\/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.base\/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 1 in <module>",
      "    at line 1461 in save(self, path, format, mode, partitionBy, **options)",
      "    at line 1322 in __call__(self, *args)",
      "    at line 179 in deco(*a, **kw)",
      "    at line 326 in get_return_value(answer, gateway_client, target_id, name)",
      "Py4JJavaError: An error occurred while calling o350.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https:\/\/spark.apache.org\/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base\/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.base\/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ouHrMWA1sTGmPTi9RNd1yy",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"eznVctmBMOtBMThherLfgG",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Version 2"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Version 2",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "from delta import *"
   ],
   "execution_count":1,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"bquZvOmWyn7s69baKBdo8A",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "SPARK_CONFIGS = {\n",
    "    \"spark.sql.sources.partitionOverwriteMode\": \"dynamic\",\n",
    "}"
   ],
   "execution_count":2,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"ArnZ5iGjEdBUAPl0IihmCm",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Global variables or constants\n",
    "INPUT_PATH = \"\/data\/workspace_files\/input\/\"\n",
    "OUTPUT_PATH = \"\/data\/workspace_files\/output\/\""
   ],
   "execution_count":3,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"V14W4Se13S4nZrgn7Yuaru",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# configuring the logger to print logs\n",
    "log_format1 = '%(asctime)s [%(levelname)-8s] <PID %(process)d:%(processName)s> %(name)s.%(funcName)s: %(message)s'\n",
    "log_format2 = '%(asctime)s [%(levelname)-8s] %(name)s.%(funcName)s: %(message)s'\n",
    "log_format3 = '%(asctime)s [%(levelname)-8s] [%(processName)s] %(name)s: %(message)s'\n",
    "\n",
    "formatter = logging.Formatter(log_format3, datefmt='%d-%b-%Y %H:%M:%S')\n",
    "\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler(OUTPUT_PATH + '\/logs\/student_etl_pyspark_stdout.log', \"a\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger(\"Students Data ETL\")\n",
    "\n",
    "logger.handlers.clear()\n",
    "\n",
    "logger.addHandler(console)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ],
   "execution_count":4,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"h7VeTpAC2KjnntPj3wRQKs",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"PySpark Excercises\") \\\n",
    "                    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\") \\\n",
    "                    .config(\"spark.sql.warehouse.dir\", \"\/data\/workspace_files\/warehouse\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ],
   "execution_count":5,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"IPStgxOODhI6Dunr1awEky",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "for key in SPARK_CONFIGS:\n",
    "    spark.conf.set(key, SPARK_CONFIGS[key])"
   ],
   "execution_count":6,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"S2ig5PlEbaSQfOlO5jCZye",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## 01. Read Data from Files"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Fq248QSFteJIkOhj2Y6l5P",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "classes_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/classes.json\")\n",
    "students_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/students_data.json\")\n",
    "address_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/student_address*.json\")\n",
    "phone_df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(INPUT_PATH + \"students\/student_phone.json\")\n",
    "marks_df = spark.read.csv(INPUT_PATH + \"students\/student_marks.csv\", header=True, schema=\"student_id long, class_id long, subject string, marks_obtained integer\")"
   ],
   "execution_count":7,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"4BqVFVYSpycsrxgcPqMVfO",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## 02. Transform Data"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"68bbyWMnT23VwP27sa36UP",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "marks_df = marks_df.groupBy(marks_df.student_id, marks_df.class_id) \\\n",
    "                    .agg( \\\n",
    "                         sum(marks_df.marks_obtained).alias(\"total_marks_obtained\"), \\\n",
    "                         sum(lit(100)).alias(\"total_marks\"), \\\n",
    "                         collect_set(struct( \\\n",
    "                                            marks_df.subject, \\\n",
    "                                            marks_df.marks_obtained \\\n",
    "                                           )).alias(\"progress_report\") \\\n",
    "                        )\n",
    "phone_df = phone_df.withColumnRenamed(\"type\", \"phone_type\")\n",
    "address_df = address_df.withColumnRenamed(\"type\", \"address_type\")"
   ],
   "execution_count":8,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"WcsTqtkBRczXvhA4wedGr1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df = students_df.join(phone_df, [students_df.student_id == phone_df.student_id], \"left\") \\\n",
    "                      .join(address_df, [students_df.student_id == address_df.student_id], \"left\") \\\n",
    "                      .join(marks_df, [students_df.student_id == marks_df.student_id], \"left\") \\\n",
    "                      .join(classes_df, [marks_df.class_id == classes_df.class_id], \"left\") \\\n",
    "                      .drop(phone_df.student_id, address_df.student_id, marks_df.student_id, classes_df.class_id)"
   ],
   "execution_count":9,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"vQkJbUtAIWJA32C9Mr9oUe",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df = final_df.groupBy(final_df.student_id) \\\n",
    "                    .agg( \\\n",
    "                             struct( \\\n",
    "                                    first(final_df.first_name, ignorenulls=True).alias(\"firstName\"), \\\n",
    "                                    first(final_df.last_name, ignorenulls=True).alias(\"lastName\") \\\n",
    "                                   ).alias(\"name\"), \\\n",
    "                             first(final_df.date_of_birth, ignorenulls=True).alias(\"dateOfBirth\"), \\\n",
    "                             first(final_df.gender, ignorenulls=True).alias(\"gender\"), \\\n",
    "                             first(final_df.hair_color, ignorenulls=True).alias(\"hairColor\"), \\\n",
    "                             first(final_df.eye_color, ignorenulls=True).alias(\"eyeColor\"), \\\n",
    "                             first(final_df.height, ignorenulls=True).alias(\"height\"), \\\n",
    "                             first(final_df.weight, ignorenulls=True).alias(\"weight\"), \\\n",
    "                             collect_set(final_df.hobbies).alias(\"hobbies\"), \\\n",
    "                             first(final_df.library_card_number, ignorenulls=True).alias(\"libraryCardNumber\"), \\\n",
    "                             first(final_df.meal_plan, ignorenulls=True).alias(\"mealPlan\"), \\\n",
    "                             first(final_df.sports_team, ignorenulls=True).alias(\"spartTeam\"), \\\n",
    "                             first(final_df.club_membership, ignorenulls=True).alias(\"clubMembership\"), \\\n",
    "                             first(final_df.tuition_paid, ignorenulls=True).alias(\"isTuitionPaid\"), \\\n",
    "                             first(final_df.financial_aid, ignorenulls=True).alias(\"isFinancialAided\"), \\\n",
    "                             first(final_df.housing_status, ignorenulls=True).alias(\"housingStatus\"), \\\n",
    "                             collect_set(struct( \\\n",
    "                                                 final_df.phone_type.alias(\"type\"), \\\n",
    "                                                 final_df.phone_number, \\\n",
    "                                                 final_df.email \\\n",
    "                                                )).alias(\"contact_detail\"), \\\n",
    "                             collect_set(struct( \\\n",
    "                                                final_df.address_type.alias(\"type\"), \\\n",
    "                                                final_df.street_address.alias(\"address_line1\"), \\\n",
    "                                                final_df.address_line_2, \\\n",
    "                                                final_df.city, \\\n",
    "                                                final_df.state, \\\n",
    "                                                final_df.country, \\\n",
    "                                                final_df.postal_code, \\\n",
    "                                                struct(final_df.latitude, final_df.longitude).alias(\"location\") \\\n",
    "                                               )).alias(\"address\"), \\\n",
    "                             collect_set(struct( \\\n",
    "                                                final_df.standard, \\\n",
    "                                                final_df.section, \\\n",
    "                                                marks_df.total_marks_obtained, \\\n",
    "                                                ((marks_df.total_marks_obtained \/ marks_df.total_marks) * 100).cast(\"decimal(3,2)\").alias(\"percent\"), \\\n",
    "                                                marks_df.progress_report \\\n",
    "                            )).alias(\"classes\")\n",
    "                           )"
   ],
   "execution_count":10,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"dUVhkkSkEpXObzgAPlaX0F",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df = final_df.withColumnRenamed(\"student_id\", \"_id\")"
   ],
   "execution_count":11,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"iBqO4j3gCbyY53ureNoCsj",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## 03. Load data"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"77lDyFdjQMh2UKrpXTVtKa",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df.show(5, truncate=False)"
   ],
   "execution_count":12,
   "outputs":[
    {
     "ename":"Py4JJavaError",
     "evalue":"Py4JJavaError: An error occurred while calling o267.showString.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base\/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.base\/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 65 more\n",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 1 in <module>",
      "    at line 945 in show(self, n, truncate, vertical)",
      "    at line 976 in _show_string(self, n, truncate, vertical)",
      "    at line 1322 in __call__(self, *args)",
      "    at line 179 in deco(*a, **kw)",
      "    at line 326 in get_return_value(answer, gateway_client, target_id, name)",
      "Py4JJavaError: An error occurred while calling o267.showString.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base\/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.base\/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 65 more\n"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"46elfGroYqRm1450LBW4I4",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "url = \"mongodb+srv:\/\/admin:Admin1234567890@datacluster.v8sn2hp.mongodb.net\/?retryWrites=true&w=majority&appName=DataCluster\"\n",
    "database = \"dummy\"\n",
    "collection = \"students\""
   ],
   "execution_count":43,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"olY5UzKLmdTkcgF5CcDCa2",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "final_df.write.format(\"mongodb\") \\\n",
    "              .mode(\"append\") \\\n",
    "              .option(\"mode\", \"dropMalformed\") \\\n",
    "              .option(\"connection.uri\", url) \\\n",
    "              .option(\"database\", database) \\\n",
    "              .option(\"collection\", collection) \\\n",
    "              .option(\"maxBatchSize\", 10000) \\\n",
    "              .save()"
   ],
   "execution_count":47,
   "outputs":[
    {
     "ename":"Py4JJavaError",
     "evalue":"Py4JJavaError: An error occurred while calling o1179.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https:\/\/spark.apache.org\/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base\/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.base\/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 1 in <module>",
      "    at line 1461 in save(self, path, format, mode, partitionBy, **options)",
      "    at line 1322 in __call__(self, *args)",
      "    at line 179 in deco(*a, **kw)",
      "    at line 326 in get_return_value(answer, gateway_client, target_id, name)",
      "Py4JJavaError: An error occurred while calling o1179.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https:\/\/spark.apache.org\/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base\/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.base\/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"pBFEH0xo98MwShXuTMOtR2",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"YWEaOHmTwJqpJCONLOpDH1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"conda",
   "base_environment":"minimal",
   "packages":[],
   "report_row_ids":[],
   "report_tabs":[],
   "version":4
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}