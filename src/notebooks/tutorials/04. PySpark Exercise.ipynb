{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Global"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Global",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"cd53Wvapf723VrGfmhWB9r",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "SPARK_CONFIGS = {\n",
    "    \"spark.sql.sources.partitionOverwriteMode\": \"dynamic\",\n",
    "    \"spark.jars.packages\": \"io.delta:delta-core_2.13:2.4.0\",\n",
    "    \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "}"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"olL5XFTDOTAz938YjHdhqT",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Global variables or constants\n",
    "INPUT_PATH = \"\/data\/workspace_files\/input\/\"\n",
    "OUTPUT_PATH = \"\/data\/workspace_files\/output\/\"\n",
    "\n",
    "MAIL_SERVER = \"aaa.com\"\n",
    "MAIL_PORT = 0\n",
    "SENDER_ID = \"xyz@em.com\"\n",
    "RECEIVER_IDS = (\"abc@em.com\", \"\")"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"gX8NHuGNqO0S70r8Gx07pQ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "SIMPLE_EMAIL_MESSAGE = \"\"\"\n",
    "<html>\n",
    "    <h1>Test mail<\/h1>\n",
    "    <p>This is a Test mail<\/p>\n",
    "<\/html>\n",
    "\"\"\""
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"B7BbWqHWDOePY6LuUdajQm",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# configuring the logger to print logs\n",
    "log_format1 = '%(asctime)s [%(levelname)-8s] <PID %(process)d:%(processName)s> %(name)s.%(funcName)s: %(message)s'\n",
    "log_format2 = '%(asctime)s [%(levelname)-8s] %(name)s.%(funcName)s: %(message)s'\n",
    "log_format3 = '%(asctime)s [%(levelname)-8s] [%(processName)s] %(name)s: %(message)s'\n",
    "\n",
    "formatter = logging.Formatter(log_format3, datefmt='%d-%b-%Y %H:%M:%S')\n",
    "\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler(OUTPUT_PATH + '\/logs\/pyspark_stdout.log', \"a\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger(\"PySpark Exercises\")\n",
    "\n",
    "logger.handlers.clear()\n",
    "\n",
    "logger.addHandler(console)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"djPPSX1jADJtqRDuT1KTvL",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"PySpark Excercises\") \\\n",
    "                    .config(\"spark.sql.warehouse.dir\", \"\/data\/workspace_files\/warehouse\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ],
   "execution_count":1,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"Xwis5rvW5smUMe4ZN9FI17",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "for key in SPARK_CONFIGS:\n",
    "    spark.conf.set(key, SPARK_CONFIGS[key])"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"yfd0FsvZqQpPMFLTFkkyPE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def send_email(mail_from=SENDER_ID,\n",
    "               mail_to=RECEIVER_IDS,\n",
    "               subject=None,\n",
    "               message_text=None,\n",
    "               file=None\n",
    "               ):\n",
    "    mail_sent = False\n",
    "    try:\n",
    "        message = MIMEMultipart(\"alternative\")\n",
    "        message[\"Subject\"] = subject\n",
    "        message[\"From\"] = mail_from\n",
    "        message[\"To\"] = \", \".join(mail_to)\n",
    "        message.attach(MIMEText(message_text, 'html'))\n",
    "        if file is not None:\n",
    "            part = MIMEBase('application', \"octet-stream\")\n",
    "            part.set_payload(open(file, \"rb\").read())\n",
    "            encoders.encode_base64(part)\n",
    "            _, tail = os.path.split(file)\n",
    "            part.add_header('Content-Disposition',\n",
    "                            'attachment; filename={}'.format(str(tail)))\n",
    "            message.attach(part)\n",
    "        print(\"Message is : \\n\", message.as_string())\n",
    "        smtp = smtplib.SMTP(MAIL_SERVER, MAIL_PORT)\n",
    "        smtp.sendmail(mail_from, mail_to, message.as_string())\n",
    "        mail_sent = True\n",
    "        smtp.quit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return mail_sent"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"LT0yGOchGmx5n7cq0QSnWk",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Estimate the partition count \n",
    "def num_partitions(spark: SparkSession, file_size, num_of_files = 1) -> int:\n",
    "    # Check the default partition size\n",
    "    partition_size = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "    # Check the default open Cost in Bytes\n",
    "    open_cost_size = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\",\"\"))\n",
    "    # Default parallelism\n",
    "    parallelism = int(spark.sparkContext.defaultParallelism)\n",
    "    # Total Actual File Size in Bytes\n",
    "    total_file_size = file_size * num_of_files\n",
    "    # Padded file size for Spark read\n",
    "    padded_file_size = total_file_size + (num_of_files * open_cost_size)\n",
    "    # Number of Bytes per Core\n",
    "    bytes_per_core = padded_file_size \/ parallelism\n",
    "    # Max Split Bytes\n",
    "    max_bytes_per_split = min(partition_size, max(open_cost_size, bytes_per_core))\n",
    "    # Total number of Partitions\n",
    "    num_of_partitions = padded_file_size \/ max_bytes_per_split\n",
    "    \n",
    "    return num_of_partitions"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"wAV9P5qnRaiM2iMvq1SGoJ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_updated_headers_expr(df: DataFrame) -> list:\n",
    "    fixed_col_list: list = []\n",
    "    for col in df.columns:\n",
    "        fixed_col_list.append(f\"`{str(col).strip()}` as {str(col).strip().replace(' ','_').lower()}\")\n",
    "        \n",
    "    return fixed_col_list"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"TKDrGNb5rRsHrGNvA2bDHk",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def fix_header(df: DataFrame):\n",
    "    n_df = df\n",
    "    for col in df.columns:\n",
    "        n_df = df.withColumnRenamed(col, str(col).strip().replace(' ','_').lower())\n",
    "    return n_df"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"6wgPISqWwitzWEutmkkMLd",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Create outer method to return the flattened Data Frame\n",
    "def flatten_json_df(_df: DataFrame) -> DataFrame:\n",
    "    # List to hold the dynamically generated column names\n",
    "    flattened_col_list = []\n",
    "    \n",
    "    # Inner method to iterate over Data Frame to generate the column list\n",
    "    def get_flattened_cols(df: DataFrame, struct_col: str = None) -> None:\n",
    "        for col in df.columns:\n",
    "            if df.schema[col].dataType.typeName() != 'struct':\n",
    "                if struct_col is None:\n",
    "                    flattened_col_list.append(f\"{col} as {col.replace('.','_')}\")\n",
    "                else:\n",
    "                    t = struct_col + \".\" + col\n",
    "                    flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
    "            else:\n",
    "                chained_col = struct_col +\".\"+ col if struct_col is not None else col\n",
    "                get_flattened_cols(df.select(col+\".*\"), chained_col)\n",
    "    \n",
    "    # Call the inner Method\n",
    "    get_flattened_cols(_df)\n",
    "    \n",
    "    # Return the flattened Data Frame\n",
    "    return _df.selectExpr(flattened_col_list)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"dqVVZWlzmExNRWBnrvAxHl",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def union_unmachted_cols_df(df_1: DataFrame, df_2: DataFrame) -> DataFrame:\n",
    "    # Lets add missing columns from df_2 to df_1\n",
    "    for col in df_2.columns:\n",
    "        if col not in df_1.columns:\n",
    "            df_1 = df_1.withColumn(col, lit(None))\n",
    "    # Lets add missing columns from df_1 to df_2\n",
    "    for col in df_1.columns:\n",
    "        if col not in df_2.columns:\n",
    "            df_2 = df_2.withColumn(col, lit(None))\n",
    "    return df_1.unionByName(df_2)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"rn6mpOjmcVdmmEOKz3ZhPW",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_values_as_list(df:Dataframe, column_name:str) -> list:\n",
    "    return df.select(column_name).distinct().rdd.map(lambda x: x[0]).collect()"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"ZpnWVfE5z1KENpMWTKyyuT",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def list_to_str(data_list:list):\n",
    "    return ', \\n'.join(data_list)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"rSN6i3FWt4GZRYypbyNaR2",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Exercise 1"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Exercise 1",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Objective – Explore data using Pyspark\n",
    "\n",
    "### Tasks\n",
    "1. Code to read data from file or databse and do the data exploration:-\n",
    "    1. Type inference: detect the types of columns in a data frame.\n",
    "    2. Essentials: type, unique values, missing values\n",
    "    3. Quantile statistics like minimum value, Q1, median, Q3, maximum, range, inter-quartile range\n",
    "    4. Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n",
    "    5. Most frequent values\n",
    "2. Write the code in such a way that if dataset is chnaged we can still explore the given statistics.\n",
    "\n",
    "### Inputs\n",
    "- Any type of data\n",
    "### Outputs\n",
    "- Standard output i.e. either in file i.e. excel(preferred) or csv or in table"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"oPO79e0QahFTLpIxo4RJWB",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "DATA_TYPES = {\n",
    "    \"numeric\" : [],\n",
    "    \"string\": [],\n",
    "    \"date_time\": []\n",
    "}"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"K1ath1QfbrvOlaDuiHExjn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "summary_schema = StructType([ \\\n",
    "                             StructField(\"column_name\", StringType(), False), \\\n",
    "                             StructField(\"data_type\", StringType(), False), \\\n",
    "                             StructField(\"expected_data_type\", StringType(), False), \\\n",
    "                             StructField(\"missing_val_count\", StringType(), False), \\\n",
    "                             StructField(\"unique_val_count\", StringType(), False), \\\n",
    "                             StructField(\"min\", StringType(), False), \\\n",
    "                             StructField(\"max\", StringType(), False), \\\n",
    "                             StructField(\"mean\", StringType(), False), \\\n",
    "                             StructField(\"median\", StringType(), False), \\\n",
    "                             StructField(\"mode\", StringType(), False), \\\n",
    "                             StructField(\"std_dev\", StringType(), False), \\\n",
    "                             StructField(\"q01\", StringType(), False), \\\n",
    "                             StructField(\"q25\", StringType(), False), \\\n",
    "                             StructField(\"q50\", StringType(), False), \\\n",
    "                             StructField(\"q75\", StringType(), False), \\\n",
    "                             StructField(\"q95\", StringType(), False), \\\n",
    "                             StructField(\"q99\", StringType(), False), \\\n",
    "                             StructField(\"kurtosis\", StringType(), False), \\\n",
    "                             StructField(\"skewness\", StringType(), False), \\\n",
    "                             StructField(\"median_abs_dev\", StringType(), False), \\\n",
    "                             StructField(\"coeff_of_var\", StringType(), False) \\\n",
    "                            ])"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"DexQPxDb7dRiNBVYryRyPS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def explore_data(spark, path, schema = None, data_limit = None):\n",
    "    summary_df = spark.createDataFrame([], )\n",
    "    data_df = None\n",
    "    # 1. read data\n",
    "    if \"\/\" in path:\n",
    "        data_df = spark.read.load(path)\n",
    "    else:\n",
    "        data_df = spark.table(path)\n",
    "    if schema is not None:\n",
    "        data_df = data_df.selectExpr(schema)\n",
    "    if data_limit is not None:\n",
    "        data_df = data_df.limit(data_limit)\n",
    "    # 2. get the column names and there default data types\n",
    "    columns = df.columns\n",
    "    types = df.dtypes\n",
    "    # 3. detect the suitable type for the column\n",
    "    for col in columns:\n",
    "        pass\n",
    "    # 4. find the missing and unique value count for each and every column\n",
    "    # 5. find min, max, mean, median, mode, quantiles, std dev, \n",
    "    stats_df = df.summary()\n",
    "    # 6. find frequently used values i.e. top 5 - 10\n",
    "    # 7. find median absolute deviation, coefficient of variation, kurtosis, skewness\n",
    "    # 8. Merge the output from step 2 to 7 into one dataframe (for wirting and displaying) and return that dataframe\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"0LQliuyJ7rxEVFpXJnLYR8",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def process_data():\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"ZhxAujgfENGL905egqsI0f",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Exercise 2"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Exercise 2",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Objective – Incremental Load of Data (not using the Delta format)\n",
    "\n",
    "### Tasks\n",
    "\n",
    "### Inputs\n",
    "\n",
    "### Output"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"rtXnD46OyZLfULyAWUfaYX",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_order_related_data():\n",
    "    data1 = [\n",
    "        [\"ORD1001\", \"P003\", 70, \"01-21-2022\", \"01-30-2022\"],\n",
    "        [\"ORD1004\", \"P033\", 12, \"01-24-2022\", \"01-30-2022\"],\n",
    "        [\"ORD1005\", \"P036\", 10, \"01-20-2022\", \"01-30-2022\"],\n",
    "        [\"ORD1002\", \"P016\", 2, \"01-10-2022\", \"01-30-2022\"],\n",
    "        [\"ORD1003\", \"P012\", 6, \"01-10-2022\", \"01-30-2022\"],\n",
    "    ]\n",
    "    data2 = [\n",
    "        [\"ORD1002\", \"P016\", 16, \"01-10-2022\", \"01-31-2022\"],\n",
    "        [\"ORD1011\", \"P076\", 21, \"01-20-2022\", \"01-31-2022\"],\n",
    "    ]\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"0Ll0HmPX8VAi2G0F6RKJqc",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Excersice 3"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Excersice 3",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Objective – Delta Table Operations\n",
    "\n",
    "### Tasks\n",
    "\n",
    "### Inputs\n",
    "\n",
    "### Output"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"91vRf3SlgMNFKA7kzIt9zy",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# TODO: Complete the delta format related functions"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"syuwrR8ayVc8cCAXWFV0jG",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def create_delta_table():\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"Ni1HnbcTGxpcZFMJzuol8N",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def insert_data_delta():\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"z7FawziuOHuinz4HsYLW71",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def delete_data_delta():\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"16be15FXrMn62yqoTfRdIB",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def upsert_data_delta():\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"qbFPjqqdTX6HlsbjxBHql5",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def read_delta_table():\n",
    "    pass"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"EaTIol9xtxDpViyqDXupQC",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"iDwe117sdPNeh7xLhn90BY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# References"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"References",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "1. https:\/\/sparkbyexamples.com\/spark\/spark-performance-tuning\/\n",
    "2. https:\/\/spark.apache.org\/docs\/latest\/tuning.html#level-of-parallelism\n",
    "3. https:\/\/pub.towardsai.net\/pyspark-job-optimization-techniques-cc85be13af26\n",
    "\n",
    "\n",
    "https:\/\/www.bing.com\/search?pglt=2081&q=pyspark+optimization&cvid=0c444541cee94eebb0ff267f75e703c0&aqs=edge.3.69i57j0l8j69i11004.6528j0j1&FORM=ANAB01&PC=U531&ntref=1"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"0R3I6any83JqnX7k6yXRKk",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"lKKwMUGdV6SjkEcPAopgJR",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[],
   "report_row_ids":[],
   "report_tabs":[],
   "version":4
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}